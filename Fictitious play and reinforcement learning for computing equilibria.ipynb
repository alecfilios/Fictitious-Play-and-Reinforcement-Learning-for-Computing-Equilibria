{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb6a42f",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1 About\n",
    "\n",
    "In game theory, the problem of finding an equilibrium strategy for each player in a non-cooperative game is of great importance. Equilibrium strategies are those strategies that, when played by all players, result in a stable outcome where no player can improve their payoff by unilaterally changing their strategy. Two well-known algorithms for finding equilibrium strategies are fictitious play and reinforcement learning.\n",
    "\n",
    "\n",
    "## 1.2 Description\n",
    "\n",
    "Fictitious play is a learning algorithm that has been widely used in game theory to find equilibrium strategies in repeated games. The algorithm works by iteratively updating each player's strategy based on the observed strategies of the other players. The name \"fictitious play\" comes from the fact that the algorithm assumes that the other players' strategies are fixed, although in reality they may also be changing. The algorithm has been shown to converge to a Nash equilibrium, which is a type of equilibrium strategy where no player can improve their payoff by unilaterally changing their strategy, assuming that the other players strategies are stationary.\n",
    "\n",
    "Reinforcement learning, on the other hand, is a general learning framework that has been applied to various fields such as robotics, control, and artificial intelligence. Reinforcement learning is based on the idea that an agent interacts with an environment and learns to optimize its behavior by receiving rewards or penalties. In the context of games, reinforcement learning can be used to find equilibrium strategies by training agents to play against each other and learn from the rewards or penalties of the game.\n",
    "\n",
    "## 1.3 Goal\n",
    "\n",
    "The goal of this assignment is to explore the use of fictitious play and reinforcement learning for computing equilibrium strategies in repeated games. The assignment will involve implementing and experimenting with both algorithms and comparing their performance through experimental results. The assignment will also include a theoretical analysis of the properties of the algorithms, such as their convergence and termination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc8f71",
   "metadata": {},
   "source": [
    "# 2. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e1767",
   "metadata": {},
   "source": [
    "## 2.1 Fictitious play\n",
    "\n",
    "Fictitious play is a learning algorithm that has been widely used in game theory to find equilibrium strategies in repeated games. The algorithm works by iteratively updating each player's strategy based on the observed strategies of the other players. The name \"fictitious play\" comes from the fact that the algorithm assumes that the other players' strategies are fixed, although in reality they may also be changing.\n",
    "\n",
    "The algorithm starts with an initial strategy for each player and iteratively updates each player's strategy based on the observed strategies of the other players. The update rule for player i is as follows:\n",
    "\n",
    "$p_{i,t+1}(s) = \\frac{\\sum_{t'=1}^t[s_{-i,t'}=s]}{t}$\n",
    "\n",
    "where $p_{i,t}(s)$ is the probability of player i playing strategy s at time t, and $s_{-i,t}$ is the strategy profile of the other players at time t.\n",
    "\n",
    "The algorithm has been shown to converge to a Nash equilibrium, which is a type of equilibrium strategy where no player can improve their payoff by unilaterally changing their strategy, assuming that the other players strategies are stationary.\n",
    "\n",
    "In this assignment, we will be using Fictitious play algorithm to train agents to play a repeated game and find equilibrium strategies. The implementation will involve creating a simulation of the game and training the agents using the Fictitious play algorithm. The agents will be trained to play against each other and learn from the strategies of the other players.\n",
    "\n",
    "It's worth to mention that there are other variations of the fictitious play algorithm such as fictitious play with adaptive exploration, which uses an adaptive exploration to avoid the problem of getting stuck in a local equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a276f8",
   "metadata": {},
   "source": [
    "## 2.2 Reinforcement learning\n",
    "\n",
    "Reinforcement learning (RL) is a type of machine learning that focuses on training agents to make decisions by interacting with an environment and receiving rewards or penalties. In the context of games, RL can be used to find equilibrium strategies by training agents to play against each other and learn from the rewards or penalties of the game.\n",
    "\n",
    "One popular algorithm for RL in games is Q-learning. Q-learning is a model-free algorithm that estimates the value of each action in a given state. The agent starts with an initial estimate of the value of each action and updates it as it experiences new states and rewards. The agent uses the following formula to update its Q-values:\n",
    "\n",
    "$Q(s,a) = Q(s,a) + \\alpha(r + \\gamma \\max_{a'} Q(s',a') - Q(s,a))$\n",
    "\n",
    "where $Q(s,a)$ is the estimate of the value of taking action a in state s, $\\alpha$ is the learning rate, $r$ is the reward received after taking action a in state s, $\\gamma$ is the discount factor, and $s'$ is the resulting state after taking action a in state s.\n",
    "\n",
    "Another popular algorithm for RL in games is SARSA. SARSA is similar to Q-learning but it estimates the value of taking the action chosen by the agent in the next state rather than the action with the highest value. The agent uses the following formula to update its Q-values:\n",
    "\n",
    "$Q(s,a) = Q(s,a) + \\alpha(r + \\gamma Q(s',a') - Q(s,a))$\n",
    "\n",
    "where $Q(s,a)$ is the estimate of the value of taking action a in state s, $\\alpha$ is the learning rate, $r$ is the reward received after taking action a in state s, $\\gamma$ is the discount factor, $s'$ is the resulting state after taking action a in state s, and $a'$ is the action chosen by the agent in state $s'$.\n",
    "\n",
    "In this assignment, we will be using Q-learning and SARSA algorithms to train agents to play a repeated game and find equilibrium strategies. The implementation will involve creating a simulation of the game and training the agents using the Q-learning and SARSA algorithms. The agents will be trained to play against each other and learn from the rewards or penalties of the game.\n",
    "\n",
    "It's worth to mention that these are just a few examples of RL algorithms, there are many others algorithms that could be used such as SARSA($\\lambda$), actor-critic, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f457d",
   "metadata": {},
   "source": [
    "# 3. Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d5a001",
   "metadata": {},
   "source": [
    "## 3.1 The Prisoner's Dilemma\n",
    "\n",
    "The Prisoner's Dilemma is a classic example of a repeated non-zero sum game that has been widely studied in game theory. The game models a scenario where two players are arrested and are held in separate cells, unable to communicate with each other. The prosecutor can either charge them with a light crime (cooperate) or a heavy crime (defect). If both players cooperate, they will each receive a light sentence. However, if one player defects while the other cooperates, the defector will receive a reduced sentence while the cooperator will receive a heavy sentence. If both players defect, they will both receive a heavy sentence.\n",
    "\n",
    "The payoff matrix for the Prisoner's Dilemma is as follows:\n",
    "\n",
    "       C      D\n",
    "C  [ R, R ] [ S, T ]\n",
    "D  [ T, S ] [ P, P ]\n",
    "\n",
    "\n",
    "Where R, S, T and P are the payoffs for each player. R is the reward for both players cooperating, T is the temptation for one player to defect, S is the sucker's payoff for one player cooperating and the other defecting, and P is the punishment for both players defecting.\n",
    "\n",
    "The goal of this problem is to find the equilibrium strategies for the two players in the Prisoner's Dilemma game by using the algorithms of fictitious play and reinforcement learning. Specifically, we will be implementing and experimenting with both algorithms and comparing their performance through experimental results. The assignment will also include a theoretical analysis of the properties of the algorithms, such as their convergence and termination.\n",
    "\n",
    "This problem is of great importance in understanding the behavior of agents in non-cooperative scenarios, as well as showing the performance of the algorithms in finding the equilibrium strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4bda4",
   "metadata": {},
   "source": [
    "### 3.1.1 Solving The Prisoner's Dilemma using Fictitious play algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c6d27d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'R' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Payoff matrix for the Prisoner's Dilemma game\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m payoffs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[43mR\u001b[49m, R], [T, S]])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize strategies for both players\u001b[39;00m\n\u001b[1;32m      7\u001b[0m p1_strategy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'R' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Payoff matrix for the Prisoner's Dilemma game\n",
    "payoffs = np.array([[R, R], [T, S]])\n",
    "\n",
    "# Initialize strategies for both players\n",
    "p1_strategy = np.array([0.5, 0.5])\n",
    "p2_strategy = np.array([0.5, 0.5])\n",
    "\n",
    "# Number of iterations for the algorithm\n",
    "num_iterations = 100\n",
    "\n",
    "# Fictitious play algorithm\n",
    "for t in range(num_iterations):\n",
    "    # Player 1's strategy\n",
    "    p1_strategy = np.dot(p2_strategy, payoffs) / np.sum(np.dot(p2_strategy, payoffs))\n",
    "    # Player 2's strategy\n",
    "    p2_strategy = np.dot(p1_strategy, payoffs.T) / np.sum(np.dot(p1_strategy, payoffs.T))\n",
    "\n",
    "# Equilibrium strategies\n",
    "print(\"Player 1's equilibrium strategy:\", p1_strategy)\n",
    "print(\"Player 2's equilibrium strategy:\", p2_strategy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bafe73",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "In this code, the payoffs matrix is defined with the payoffs for the game, and the initial strategies for both players are set to [0.5, 0.5], which corresponds to a random choice between C and D. The algorithm performs the update rule for each player's strategy in each iteration and the number of iterations is set to 100. The final equilibrium strategies for each player are printed at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84051303",
   "metadata": {},
   "source": [
    "### 3.1.2 Solving The Prisoner's Dilemma using Reinforcement learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Payoff matrix for the Prisoner's Dilemma game\n",
    "payoffs = np.array([[R, R], [T, S]])\n",
    "\n",
    "# Q-values for both players\n",
    "q_values = np.zeros((2, 2))\n",
    "\n",
    "# Initialize strategies for both players\n",
    "p1_strategy = np.array([0.5, 0.5])\n",
    "p2_strategy = np.array([0.5, 0.5])\n",
    "\n",
    "# Parameters for Q-learning\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "num_iterations = 100\n",
    "\n",
    "# Q-learning algorithm\n",
    "for t in range(num_iterations):\n",
    "    # Select actions for both players\n",
    "    p1_action = np.random.choice(2, p=p1_strategy)\n",
    "    p2_action = np.random.choice(2, p=p2_strategy)\n",
    "    \n",
    "    # Update Q-values for both players\n",
    "    q_values[p1_action, p2_action] = (1 - alpha) * q_values[p1_action, p2_action] + alpha * (payoffs[p1_action, p2_action] + gamma * np.max(q_values))\n",
    "    q_values[p2_action, p1_action] = (1 - alpha) * q_values[p2_action, p1_action] + alpha * (payoffs[p1_action, p2_action] + gamma * np.max(q_values))\n",
    "    \n",
    "    # Update strategies for both players\n",
    "    p1_strategy = np.exp(q_values[:, p2_action]) / np.sum(np.exp(q_values[:, p2_action]))\n",
    "    p2_strategy = np.exp(q_values[p1_action, :]) / np.sum(np.exp(q_values[p1_action, :]))\n",
    "\n",
    "# Equilibrium strategies\n",
    "print(\"Player 1's equilibrium strategy:\", p1_strategy)\n",
    "print(\"Player 2's equilibrium strategy:\", p2_strategy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0036f",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "In this code, the payoffs matrix is defined with the payoffs for the game, and the initial strategies for both players are set to [0.5, 0.5], which corresponds to a random choice between C and D. The algorithm performs the update rule for each player's strategy in each iteration and the number of iterations is set to 100. The final equilibrium strategies for each player are printed at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e2d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f317fa18",
   "metadata": {},
   "source": [
    "# Fictitious play and reinforcement learning for computing equilibria\n",
    "\n",
    "## Intelligent Agents and Multiagent Systems Project \n",
    "\n",
    "## NCSR Demokritos 2022-2023\n",
    "\n",
    "### Authors: \n",
    "\n",
    "- ##  Alexandros Filios - mtn2219 \n",
    "- ##  Nikolaos Chiotis - mtn2221 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb6a42f",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1 About\n",
    "\n",
    "In game theory, the problem of finding an equilibrium strategy for each player in a non-cooperative game is of great importance. Equilibrium strategies are those strategies that, when played by all players, result in a stable outcome where no player can improve their payoff by unilaterally changing their strategy. Two well-known techniques for finding equilibrium strategies are fictitious play and reinforcement learning.\n",
    "\n",
    "\n",
    "## 1.2 Description\n",
    "\n",
    "In this project we are going to implement and analyze several examples of repeated & zero sum (stochastic) games. We are going to use, as mentioned above, Fictitious play and an RL algorithm.\n",
    "\n",
    "Fictitious play is a learning algorithm that has been widely used in game theory to find equilibrium strategies in repeated games. The algorithm works by iteratively updating each player's strategy based on the observed strategies of the other players. The name \"fictitious play\" comes from the fact that the algorithm assumes that the other players' strategies are fixed, although in reality they may also be changing. The algorithm has been shown to converge to a *Nash equilibrium*, which is a type of equilibrium strategy where no player can improve their payoff by unilaterally changing their strategy, assuming that the other players strategies are stationary.\n",
    "\n",
    "Reinforcement learning, on the other hand, is a general learning framework that has been applied to various fields such as robotics, control, and artificial intelligence. Reinforcement learning is based on the idea that an agent interacts with an environment and learns to optimize its behavior by receiving rewards or penalties(negative rewards). In the context of games, reinforcement learning can be used to find equilibrium strategies by training agents to play against each other and learn from the rewards or penalties of the game.\n",
    "\n",
    "## 1.3 Goal\n",
    "\n",
    "The goal of this assignment is to explore the use of fictitious play and reinforcement learning for computing equilibrium strategies in repeated games. The assignment will involve implementing and experimenting with both algorithms and comparing their performance through experimental results. The assignment will also include a theoretical analysis of the properties of the algorithms, such as their convergence and termination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc8f71",
   "metadata": {},
   "source": [
    "# 2. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e1767",
   "metadata": {},
   "source": [
    "## 2.1 Fictitious play\n",
    "\n",
    "The algorithm starts with an initial strategy for each player and iteratively updates each player's strategy based on the observed strategies of the other players. The update rule for player i is as follows:\n",
    "\n",
    "$$ P(a) = \\frac{w(a)}{\\sum_{a'\\in A}w(a')} $$\n",
    "\n",
    "where \n",
    "\n",
    "- $P(a)$ is the probability of an agent taking action $a$.\n",
    "- $w(a)$ is the number of occasions action $a$ apperared so far.\n",
    "- $\\sum_{a'\\in A}w(a')$ is the sum of all actions, inside the available action vector A, that occured so far.\n",
    "\n",
    "The algorithm has been shown to converge to a Nash equilibrium, which is a type of equilibrium strategy where no player can improve their payoff by unilaterally changing their strategy, assuming that the other players strategies are stationary.\n",
    "\n",
    "In this assignment, we will be using Fictitious play algorithm to train agents to play a repeated game and find equilibrium strategies. The implementation will involve creating simulations of the game and training the agents using the Fictitious play algorithm. The agents will be trained to play against each other and learn from the strategies of the other players.\n",
    "\n",
    "It's worth to mention that there are other variations of the fictitious play algorithm such as fictitious play with adaptive exploration, which uses an adaptive exploration to avoid the problem of getting stuck in a local equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a276f8",
   "metadata": {},
   "source": [
    "## 2.2 Reinforcement learning\n",
    "\n",
    "Reinforcement learning (RL) is a type of machine learning that focuses on training agents to make decisions by interacting with an environment and receiving rewards or penalties. In the context of games, RL can be used to find equilibrium strategies by training agents to play against each other and learn from the rewards or penalties of the game.\n",
    "\n",
    "One popular algorithm for RL in games is Q-learning. Q-learning is a model-free algorithm that estimates the value of each action in a given state. The agent starts with an initial estimate of the value of each action and updates it as it experiences new states and rewards. The agent uses the following formula to update its Q-values:\n",
    "\n",
    "$Q(s,a) = Q(s,a) + \\alpha(r + \\gamma \\max_{a'} Q(s',a') - Q(s,a))$\n",
    "\n",
    "where $Q(s,a)$ is the estimate of the value of taking action a in state s, $\\alpha$ is the learning rate, $r$ is the reward received after taking action a in state s, $\\gamma$ is the discount factor, and $s'$ is the resulting state after taking action a in state s.\n",
    "\n",
    "Another popular algorithm for RL in games is SARSA. SARSA is similar to Q-learning but it estimates the value of taking the action chosen by the agent in the next state rather than the action with the highest value. The agent uses the following formula to update its Q-values:\n",
    "\n",
    "$Q(s,a) = Q(s,a) + \\alpha(r + \\gamma Q(s',a') - Q(s,a))$\n",
    "\n",
    "where $Q(s,a)$ is the estimate of the value of taking action a in state s, $\\alpha$ is the learning rate, $r$ is the reward received after taking action a in state s, $\\gamma$ is the discount factor, $s'$ is the resulting state after taking action a in state s, and $a'$ is the action chosen by the agent in state $s'$.\n",
    "\n",
    "In this assignment, we will be using Q-learning and SARSA algorithms to train agents to play a repeated game and find equilibrium strategies. The implementation will involve creating a simulation of the game and training the agents using the Q-learning and SARSA algorithms. The agents will be trained to play against each other and learn from the rewards or penalties of the game.\n",
    "\n",
    "It's worth to mention that these are just a few examples of RL algorithms, there are many others algorithms that could be used such as SARSA($\\lambda$), actor-critic, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f457d",
   "metadata": {},
   "source": [
    "# 3. Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d5a001",
   "metadata": {},
   "source": [
    "## 3.1 Rock Paper Scissors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4bda4",
   "metadata": {},
   "source": [
    "### 3.1.1 Solving the Rock Paper Scissors game using Fictitious play algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c4e2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7dce175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_an_action(probability_matrix):\n",
    "    # The belief comes from the most probable opponent action so far\n",
    "    belief = np.argmax(probability_matrix)\n",
    "    # if the belief indicates that the opponent chose rock \n",
    "    if belief==0:\n",
    "        # then choose paper\n",
    "        action=1\n",
    "    # if the belief indicates that the opponent chose paper \n",
    "    elif belief==1:\n",
    "        # then choose scissors\n",
    "        action=2\n",
    "    # if the belief indicates that the opponent chose scissors \n",
    "    else:\n",
    "        # then choose rock\n",
    "        action=0\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "800b20ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1:  [0.55474106 0.30412653 0.14113241]\n",
      "Player 2:  [0.20727598 0.34653333 0.44619069]\n"
     ]
    }
   ],
   "source": [
    "payoff_matrix = np.array([[(0,0),(-1,1),(1,-1)],\n",
    "                          [(1,-1),(0,0),(-1,1)],\n",
    "                          [(-1,1),(1,-1),(0,0)]])\n",
    "\n",
    "# Define the number of iterations to run the algorithm\n",
    "num_iterations = 100\n",
    "\n",
    "# Define the number of rounds for each iteration\n",
    "num_rounds = 10000\n",
    "\n",
    "rewards_p1 = np.zeros(num_iterations)\n",
    "rewards_p2 = np.zeros(num_iterations)\n",
    "\n",
    "# Run the algorithm for the specified number of iterations\n",
    "for i in range(num_iterations):\n",
    "    temp_reward_1 = 0\n",
    "    temp_reward_2 = 0\n",
    "\n",
    "    # Initialize the number of times each action has been taken\n",
    "    # TODO: Note: the prior beliefs cannot be all 0 since this does not define a meaningful mixed strategy\n",
    "    w1 = np.random.uniform(low=0.5, high=2.5, size=(3,)).round(3) # Number of times P1 took each of the 9 positions in the game\n",
    "    w2 = np.random.uniform(low=0.5, high=2.5, size=(3,)).round(3) # Number of times P2 took each of the 9 positions in the game\n",
    "\n",
    "    # Define the initial action probabilities for both agents\n",
    "    # TODO: Make them random on init\n",
    "    p1 = w1 / np.sum(w1)\n",
    "    p2 = w2 / np.sum(w2)\n",
    "    \n",
    "    for j in range(num_rounds):\n",
    "        \n",
    "        a1 = choose_an_action(p1)\n",
    "        \n",
    "        a2 = choose_an_action(p2)\n",
    "\n",
    "        # Update the number of times each opponent action has been taken\n",
    "        w1[a2] += 1\n",
    "        w2[a1] += 1\n",
    "        \n",
    "        # Update the action probabilities for both agents\n",
    "        p1 = w1 / np.sum(w1)\n",
    "        p2 = w2 / np.sum(w2)\n",
    "    \n",
    "        temp_reward_1 += payoff_matrix[a1][a2][0]\n",
    "        temp_reward_2 += payoff_matrix[a1][a2][1]\n",
    "    \n",
    "    rewards_p1[i] = temp_reward_1\n",
    "    rewards_p2[i] = temp_reward_2\n",
    "\n",
    "# Print the final action probabilities\n",
    "print(\"Player 1: \", p1)\n",
    "print(\"Player 2: \", p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fa7f9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1288., -1746.,     0., -1264.,  1082.,  -790., -1700.,  1447.,\n",
       "        -668.,  1144.,   522.,   997.,  -248., -1704.,   -82.,  1498.,\n",
       "        1846.,  1284.,  -228.,  1704.,  1702.,   331.,  1746.,  1272.,\n",
       "        1144.,  -376.,  1284., -1790.,  -376.,  -730.,  1846.,  -660.,\n",
       "        -375.,     0., -1704.,   935.,  1556., -1516.,  1704.,  -790.,\n",
       "        -246.,  1704.,  -376., -1746., -1746.,   668.,  -668., -1082.,\n",
       "       -1516.,  -228.,  1702.,  -229., -1702., -1703.,  -522.,  -998.,\n",
       "       -1702., -1144.,  1746., -1851.,   522.,     0.,     0.,  1850.,\n",
       "        1746.,  -480.,     0.,   375., -1272.,   668.,     0.,  1704.,\n",
       "         790.,   668.,  -376., -1284.,  1144., -1229., -1702.,  1790.,\n",
       "         230.,   520.,   665.,   229.,   246.,   850., -1702., -1034.,\n",
       "         186.,  1033., -1264., -1701.,   935.,     0., -1472.,     0.,\n",
       "        -480.,   788.,     0.,  1082.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5847c2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1288.,  1746.,     0.,  1264., -1082.,   790.,  1700., -1447.,\n",
       "         668., -1144.,  -522.,  -997.,   248.,  1704.,    82., -1498.,\n",
       "       -1846., -1284.,   228., -1704., -1702.,  -331., -1746., -1272.,\n",
       "       -1144.,   376., -1284.,  1790.,   376.,   730., -1846.,   660.,\n",
       "         375.,     0.,  1704.,  -935., -1556.,  1516., -1704.,   790.,\n",
       "         246., -1704.,   376.,  1746.,  1746.,  -668.,   668.,  1082.,\n",
       "        1516.,   228., -1702.,   229.,  1702.,  1703.,   522.,   998.,\n",
       "        1702.,  1144., -1746.,  1851.,  -522.,     0.,     0., -1850.,\n",
       "       -1746.,   480.,     0.,  -375.,  1272.,  -668.,     0., -1704.,\n",
       "        -790.,  -668.,   376.,  1284., -1144.,  1229.,  1702., -1790.,\n",
       "        -230.,  -520.,  -665.,  -229.,  -246.,  -850.,  1702.,  1034.,\n",
       "        -186., -1033.,  1264.,  1701.,  -935.,     0.,  1472.,     0.,\n",
       "         480.,  -788.,     0., -1082.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_p2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bafe73",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84051303",
   "metadata": {},
   "source": [
    "### 3.1.2 Solving The Prisoner's Dilemma using Reinforcement learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e2d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abe0036f",
   "metadata": {},
   "source": [
    "### Description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
